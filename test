#!/usr/bin/env python3
"""
Consolidated AgentSpec Test Runner

This script provides comprehensive test execution capabilities including:
- Unit tests
- Integration tests
- End-to-end tests
- CLI functionality tests
- Code quality checks (linting, formatting, type checking)
- Coverage reporting
- Performance testing
- Security scanning

Default behavior runs all tests comprehensively.
"""

import argparse
import os
import shutil
import subprocess
import sys
import tempfile
from pathlib import Path
from typing import List, Optional


class Colors:
    """ANSI color codes for terminal output."""

    RED = "\033[0;31m"
    GREEN = "\033[0;32m"
    YELLOW = "\033[1;33m"
    BLUE = "\033[0;34m"
    NC = "\033[0m"  # No Color


class TestRunner:
    """Consolidated test runner for AgentSpec."""

    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.project_root = Path(__file__).parent
        self.failed_tests = []

    def log_info(self, message: str) -> None:
        """Log info message."""
        print(f"{Colors.BLUE}[INFO]{Colors.NC} {message}")

    def log_success(self, message: str) -> None:
        """Log success message."""
        print(f"{Colors.GREEN}[SUCCESS]{Colors.NC} {message}")

    def log_warning(self, message: str) -> None:
        """Log warning message."""
        print(f"{Colors.YELLOW}[WARNING]{Colors.NC} {message}")

    def log_error(self, message: str) -> None:
        """Log error message."""
        print(f"{Colors.RED}[ERROR]{Colors.NC} {message}")

    def run_command(
        self,
        cmd: List[str],
        description: str = "",
        cwd: Optional[Path] = None,
        capture_output: bool = False,
    ) -> bool:
        """Run a command and handle errors."""
        if description:
            self.log_info(f"üîÑ {description}")

        if self.verbose:
            print(f"Running: {' '.join(cmd)}")

        try:
            result = subprocess.run(
                cmd,
                check=True,
                capture_output=capture_output,
                text=True,
                cwd=cwd or self.project_root,
            )

            if not capture_output and result.stdout:
                print(result.stdout)

            return True

        except subprocess.CalledProcessError as e:
            self.log_error(f"Command failed with exit code {e.returncode}")
            if self.verbose:
                if e.stdout:
                    print("STDOUT:", e.stdout)
                if e.stderr:
                    print("STDERR:", e.stderr)
            return False

    def run_test_section(self, section_name: str, test_func) -> bool:
        """Run a test section and track results."""
        self.log_info(f"Testing: {section_name}")

        try:
            success = test_func()
            if success:
                self.log_success(f"‚úÖ {section_name}: PASSED")
                return True
            else:
                self.log_error(f"‚ùå {section_name}: FAILED")
                self.failed_tests.append(section_name)
                return False
        except Exception as e:
            self.log_error(f"‚ùå {section_name}: FAILED - {e}")
            self.failed_tests.append(section_name)
            return False

    def install_dependencies(self) -> bool:
        """Install test dependencies."""
        return self.run_command(
            ["pip", "install", "-e", ".[test,dev]"], "Installing test dependencies"
        )

    def run_unit_tests(self, coverage: bool = True) -> bool:
        """Run unit tests."""
        cmd = ["python3", "-m", "pytest", "tests/unit/"]

        if self.verbose:
            cmd.append("-v")

        if coverage:
            cmd.extend(["--cov=agentspec", "--cov-report=term-missing"])

        return self.run_command(cmd, "Running unit tests")

    def run_integration_tests(self) -> bool:
        """Run integration tests."""
        cmd = ["python3", "-m", "pytest", "tests/integration/"]

        if self.verbose:
            cmd.append("-v")

        return self.run_command(cmd, "Running integration tests")

    def run_e2e_tests(self) -> bool:
        """Run end-to-end tests."""
        cmd = ["python3", "-m", "pytest", "tests/e2e/"]

        if self.verbose:
            cmd.append("-v")

        return self.run_command(cmd, "Running end-to-end tests")

    def run_all_pytest_tests(self, coverage: bool = True) -> bool:
        """Run all pytest-based tests."""
        cmd = ["python3", "-m", "pytest", "tests/"]

        if self.verbose:
            cmd.append("-v")

        if coverage:
            cmd.extend(
                [
                    "--cov=agentspec",
                    "--cov-report=term-missing",
                    "--cov-report=html:htmlcov",
                    "--cov-report=xml:coverage.xml",
                ]
            )

        return self.run_command(cmd, "Running all pytest tests")

    def run_performance_tests(self) -> bool:
        """Run performance tests."""
        cmd = ["python3", "-m", "pytest", "tests/", "-m", "slow"]

        if self.verbose:
            cmd.append("-v")

        return self.run_command(cmd, "Running performance tests")

    def test_cli_functionality(self) -> bool:
        """Test CLI functionality."""
        tests = [
            (["python3", "-m", "agentspec", "--help"], "CLI Help"),
            (["python3", "-m", "agentspec", "list-tags"], "List Tags"),
            (["python3", "-m", "agentspec", "list-instructions"], "List Instructions"),
        ]

        for cmd, name in tests:
            if not self.run_command(cmd, f"Testing {name}", capture_output=True):
                return False

        return True

    def test_spec_generation(self) -> bool:
        """Test spec generation functionality."""
        # Generate a test spec
        if not self.run_command(
            [
                "python3",
                "-m",
                "agentspec",
                "generate",
                "--tags",
                "quality,testing",
                "--output",
                "test_spec.md",
            ],
            "Testing spec generation",
        ):
            return False

        # Validate generated spec content
        test_spec_path = self.project_root / "test_spec.md"
        if not test_spec_path.exists():
            self.log_error("Generated spec file not found")
            return False

        try:
            content = test_spec_path.read_text()
            required_content = [
                "AgentSpec - Project Specification",
                "## QUALITY GATES",
            ]

            for required in required_content:
                if required not in content:
                    self.log_error(f"Required content missing: {required}")
                    return False

            # Clean up test file
            test_spec_path.unlink()
            return True

        except Exception as e:
            self.log_error(f"Error validating spec content: {e}")
            return False

    def test_setup_script(self) -> bool:
        """Test setup script functionality."""
        # Test setup script help
        if not self.run_command(
            ["bash", "setup.sh", "--help"],
            "Testing setup script help",
            capture_output=True,
        ):
            return False

        # Test minimal setup in temporary directory
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            # Copy necessary files
            shutil.copy(self.project_root / "setup.sh", temp_path)

            # Run minimal setup
            if not self.run_command(
                ["bash", "setup.sh", "--minimal"],
                "Testing minimal setup",
                cwd=temp_path,
                capture_output=True,
            ):
                return False

            # Verify created structure
            required_dirs = ["scripts"]
            required_files = [".agentspec"]

            for dir_name in required_dirs:
                if not (temp_path / dir_name).exists():
                    self.log_error(f"Required directory not created: {dir_name}")
                    return False

            for file_name in required_files:
                if not (temp_path / file_name).exists():
                    self.log_error(f"Required file not created: {file_name}")
                    return False

        return True

    def run_linting(self) -> bool:
        """Run code linting."""
        success = True

        # Run flake8
        if not self.run_command(
            ["flake8", "agentspec/", "tests/"], "Running flake8 linting"
        ):
            success = False

        # Run mypy
        if not self.run_command(
            ["mypy", "agentspec/", "--ignore-missing-imports"],
            "Running mypy type checking",
        ):
            success = False

        return success

    def run_formatting_check(self) -> bool:
        """Check code formatting."""
        return self.run_command(
            ["black", "--check", "agentspec/", "tests/"], "Checking code formatting"
        )

    def run_security_scan(self) -> bool:
        """Run security scanning."""
        return self.run_command(
            ["bandit", "-r", "agentspec/", "--skip", "B101,B603,B607"],
            "Running security scan",
        )

    def run_import_sorting_check(self) -> bool:
        """Check import sorting."""
        return self.run_command(
            ["isort", "--check-only", "--profile=black", "agentspec/", "tests/"],
            "Checking import sorting",
        )

    def generate_coverage_report(self) -> bool:
        """Generate detailed coverage report."""
        success = self.run_command(
            [
                "python3",
                "-m",
                "pytest",
                "tests/",
                "--cov=agentspec",
                "--cov-report=html:htmlcov",
                "--cov-report=xml:coverage.xml",
                "--cov-report=term-missing",
            ],
            "Generating coverage report",
        )

        if success:
            self.log_info("üìä Coverage report generated:")
            self.log_info("  - HTML report: htmlcov/index.html")
            self.log_info("  - XML report: coverage.xml")

        return success

    def clean_artifacts(self) -> None:
        """Clean test artifacts."""
        artifacts = [
            "htmlcov",
            "coverage.xml",
            ".coverage",
            ".pytest_cache",
            "bandit-report.json",
            "test_spec.md",
        ]

        for artifact in artifacts:
            path = self.project_root / artifact
            if path.exists():
                if path.is_dir():
                    shutil.rmtree(path)
                    if self.verbose:
                        self.log_info(f"Removed directory: {artifact}")
                else:
                    path.unlink()
                    if self.verbose:
                        self.log_info(f"Removed file: {artifact}")

        # Remove __pycache__ directories recursively
        for pycache in self.project_root.rglob("__pycache__"):
            shutil.rmtree(pycache)
            if self.verbose:
                self.log_info(f"Removed: {pycache}")

        if self.verbose:
            self.log_success("Test artifacts cleaned")

    def run_comprehensive_tests(self) -> bool:
        """Run comprehensive test suite (default behavior)."""
        self.log_info("üß™ Running AgentSpec Comprehensive Test Suite")
        print("=" * 60)

        all_success = True

        # CLI and functionality tests
        if not self.run_test_section("CLI Functionality", self.test_cli_functionality):
            all_success = False

        if not self.run_test_section("Spec Generation", self.test_spec_generation):
            all_success = False

        if not self.run_test_section("Setup Script", self.test_setup_script):
            all_success = False

        # Code quality tests
        if not self.run_test_section("Code Linting", self.run_linting):
            all_success = False

        if not self.run_test_section("Code Formatting", self.run_formatting_check):
            all_success = False

        if not self.run_test_section("Import Sorting", self.run_import_sorting_check):
            all_success = False

        if not self.run_test_section("Security Scan", self.run_security_scan):
            all_success = False

        # Pytest-based tests with coverage
        if not self.run_test_section(
            "All Tests with Coverage", lambda: self.run_all_pytest_tests(coverage=True)
        ):
            all_success = False

        return all_success


def main():
    """Main test runner entry point."""
    parser = argparse.ArgumentParser(
        description="AgentSpec Consolidated Test Runner",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  ./test                                       # Run comprehensive test suite (default)
  ./test --unit                               # Run only unit tests
  ./test --integration                        # Run only integration tests
  ./test --e2e                               # Run only end-to-end tests
  ./test --lint                              # Run only linting checks
  ./test --coverage                          # Generate coverage report
  ./test --clean                             # Clean test artifacts
  ./test --install-deps                      # Install test dependencies
        """,
    )

    # Test type flags
    parser.add_argument("--unit", action="store_true", help="Run unit tests only")
    parser.add_argument(
        "--integration", action="store_true", help="Run integration tests only"
    )
    parser.add_argument("--e2e", action="store_true", help="Run end-to-end tests only")
    parser.add_argument(
        "--performance", action="store_true", help="Run performance tests only"
    )
    parser.add_argument(
        "--cli", action="store_true", help="Run CLI functionality tests only"
    )
    parser.add_argument(
        "--lint", action="store_true", help="Run linting and code quality checks only"
    )
    parser.add_argument(
        "--format", action="store_true", help="Run formatting checks only"
    )
    parser.add_argument(
        "--security", action="store_true", help="Run security scans only"
    )
    parser.add_argument(
        "--coverage", action="store_true", help="Generate coverage report only"
    )

    # Utility flags
    parser.add_argument(
        "--clean", action="store_true", help="Clean test artifacts and exit"
    )
    parser.add_argument(
        "--install-deps", action="store_true", help="Install test dependencies"
    )
    parser.add_argument(
        "--no-coverage",
        action="store_true",
        help="Disable coverage reporting for pytest tests",
    )

    # Output flags
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")

    args = parser.parse_args()

    # Initialize test runner
    runner = TestRunner(verbose=args.verbose)

    # Change to project root
    os.chdir(runner.project_root)

    print(f"{Colors.BLUE}üß™ AgentSpec Test Runner{Colors.NC}")
    print(f"Working directory: {runner.project_root}")
    print()

    # Handle utility commands first
    if args.clean:
        runner.clean_artifacts()
        return 0

    if args.install_deps:
        if not runner.install_dependencies():
            runner.log_error("Failed to install test dependencies")
            return 1
        if not any(
            [
                args.unit,
                args.integration,
                args.e2e,
                args.performance,
                args.cli,
                args.lint,
                args.format,
                args.security,
                args.coverage,
            ]
        ):
            return 0

    # Determine what tests to run
    specific_tests = any(
        [
            args.unit,
            args.integration,
            args.e2e,
            args.performance,
            args.cli,
            args.lint,
            args.format,
            args.security,
            args.coverage,
        ]
    )

    success = True

    if not specific_tests:
        # Default: run comprehensive test suite
        success = runner.run_comprehensive_tests()
    else:
        # Run specific test types
        if args.unit:
            success &= runner.run_test_section(
                "Unit Tests",
                lambda: runner.run_unit_tests(coverage=not args.no_coverage),
            )

        if args.integration:
            success &= runner.run_test_section(
                "Integration Tests", runner.run_integration_tests
            )

        if args.e2e:
            success &= runner.run_test_section("End-to-End Tests", runner.run_e2e_tests)

        if args.performance:
            success &= runner.run_test_section(
                "Performance Tests", runner.run_performance_tests
            )

        if args.cli:
            success &= runner.run_test_section(
                "CLI Functionality", runner.test_cli_functionality
            )

        if args.lint:
            success &= runner.run_test_section("Code Linting", runner.run_linting)

        if args.format:
            success &= runner.run_test_section(
                "Code Formatting", runner.run_formatting_check
            )

        if args.security:
            success &= runner.run_test_section(
                "Security Scan", runner.run_security_scan
            )

        if args.coverage:
            success &= runner.run_test_section(
                "Coverage Report", runner.generate_coverage_report
            )

    # Print final results
    print()
    print("=" * 60)

    if success:
        runner.log_success("üéâ All tests passed successfully!")
        return 0
    else:
        runner.log_error("‚ùå Some tests failed!")
        if runner.failed_tests:
            runner.log_error("Failed test sections:")
            for test in runner.failed_tests:
                runner.log_error(f"  - {test}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
